# Лабораторная работа №5.1. Проектирование объектной модели данных. Проектирование сквозного конвейера ETL на основе бизнес-кейса «Umbrella»

**Цель работы:** 
1. Развернуть среду оркестрации Apache Airflow с использованием Docker.
2. Изучить структуру и принципы работы ETL-конвейеров (DAG).
3. Спроектировать архитектуру аналитического решения.
4. Реализовать ETL-процесс получения погодных данных через API и их обработки.
5. Использовать обученную ML-модель для прогнозирования бизнес-метрик (продаж).

---

## 1. Техническое обеспечение и подготовка

Для выполнения работы используется виртуальная машина Ubuntu и Docker контейнеризация.

**1.1. Развертывание ВМ**
*   Скачайте образ [ETL+devops_26.ova]().
*   Разверните его в [VirtualBox]().

**1.2. Получение материалов**
В терминале ВМ клонируйте репозиторий с заданием в домашний каталог:
```bash
git clone https://github.com/BosenkoTM/workshop-on-ETL.git
```
*Рабочая директория проекта:* `workshop-on-ETL/business_case_umbrella_25`

**1.3. Получение API Key**
Для выполнения заданий с реальными данными зарегистрируйтесь на сайте [WeatherAPI.com](https://www.weatherapi.com/) и получите бесплатный API Key.

---

## 2. Запуск и настройка Airflow

Чтобы начать работу с примерами кода, запустите Airflow в Docker.

1.  Перейдите в папку проекта:
    ```bash
    cd ~/workshop-on-ETL/business_case_umbrella_25
    ```
2.  Соберите Docker-образ:
    ```bash
    sudo docker build -t custom-airflow:slim-2.8.1-python3.11 .
    ```
3.  Запустите сервисы (в первый раз это займет время):
    ```bash
    sudo docker compose up --build
    ```
    *Если требуется полная пересборка и очистка:*
    ```bash
    sudo docker system prune -a --volumes -f
    ```
4.  Откройте веб-интерфейс Airflow по адресу: `http://localhost:8080/`.
    *Логин/пароль (по умолчанию в docker-compose): `airflow`/`airflow`.*

**Управление:**
Чтобы остановить выполнение, используйте в терминале:
```bash
sudo docker compose down
```

---

## 3. Ход работы

### 3.1. Изучение модельного кейса
В интерфейсе Airflow найдите и активируйте DAG `01_umbrella.py`.
*   Запустите его (Trigger DAG).
*   Изучите Graph View. Опишите в отчете основные элементы интерфейса и логику работы этого модельного дага.

### 3.2. Архитектура решения
Спроектируйте верхнеуровневую архитектуру для реального кейса (`real_umbrella.py`) в [Draw.io](https://app.diagrams.net/).
Схема должна содержать слои:
*   **Source Layer:** Источник данных (WeatherAPI).
*   **Storage Layer:** Где хранятся сырые данные (CSV) и артефакты (ML-модель `.pkl`).
*   **Business Layer:** Потребители данных (Jupyter Notebook для прогноза, BI-отчеты).

*Сохраните схему как `archi.png` и включите в отчет.*

### 3.3. Настройка реального конвейера
1.  Откройте файл `dags/real_umbrella.py` в редакторе кода (на ВМ).
2.  Найдите функцию `fetch_weather_forecast` и вставьте ваш API ключ:
    ```python
    def fetch_weather_forecast():
        api_key = "ВАШ_КЛЮЧ_ЗДЕСЬ" 
        # ...
    ```
3.  Запустите DAG `real_umbrella` в Airflow. Убедитесь, что все этапы прошли успешно (зеленый цвет).
4.  В результате работы DAG должен сформироваться файл обученной модели `ml_model.pkl` (обычно сохраняется в папку `data/` или корень контейнера, проброшенный в локальную папку).

---

## 4. Индивидуальные задания (Модификация ETL)

Вам необходимо создать **новый DAG** (или модифицировать копию `real_umbrella.py` под именем `variant_XX.py`), который выполняет задания согласно вашему варианту.

| Вариант | Задание 1 (Сбор данных) | Задание 2 (Трансформация) | Задание 3 (Сохранение/Визуализация) |
|---|---|---|---|
| 1 | Прогноз: Москва, 3 дня | Оставить поля: date, avgtemp_c | Сохранить как CSV |
| 2 | Прогноз: Санкт-Петербург, 5 дней | Дата в формат YYYY-MM-DD | Построить график температуры (png) |
| 3 | Прогноз: Лондон, 7 дней | Фильтр: t > 10°C | Сохранить в CSV |
| 4 | Прогноз: Берлин, 3 дня | Рассчитать среднюю t | Вывести дату макс. t в лог |
| 5 | Прогноз: Координаты 48.85, 2.35 (Париж), 7 дней | Поля: date, avgtemp_c | Найти мин. t |
| 6 | Прогноз: Рим, 5 дней | Заменить пропуски средним | Построить Bar Chart |
| 7 | Прогноз: Мадрид, 7 дней | Столбец "тепло/холодно" (>15°C) | Подсчитать кол-во "тёплых" дней |
| 8 | Прогноз: Токио, 3 дня | JSON -> DataFrame | Сохранить в Excel |
| 9 | Прогноз: Афины, 3 дня | Округлить t | Линейный график по дням |
| 10 | Прогноз: Нью-Йорк, 3 дня | Удалить строки с NULL | Таблица: дата и температура |
| 11 | Прогноз: Лос-Анджелес, 5 дней | Сортировка t (desc) | Топ-3 жарких дня в CSV |
| 12 | Прогноз: Амстердам, 7 дней | Фильтр: t < 0°C | Вывести среднюю t |
| 13 | Прогноз: Прага, 3 дня | Столбец: дельта с пред. днём | Таблица изменений |
| 14 | Прогноз: Варшава, 5 дней | Группировка: среднее по рабочим дням | Визуализировать таблицу |
| 15 | Прогноз: Бангкок, 7 дней | Сравнить t первого и последнего дня | Print результата |
| 16 | Прогноз: Дубай, 3 дня | Фильтр: t > 30°C | Вывести кол-во таких дней |
| 17 | Прогноз: Хельсинки, 5 дней | Заполнить пропуски 0 | Сохранить в Parquet |
| 18 | Прогноз: Стокгольм, 3 дня | Удалить дубликаты | Таблица "Дата — Температура" |
| 19 | Прогноз: Вена, 5 дней | Фильтр: только выходные | Средняя t выходных |
| 20 | Прогноз: Киев, 7 дней | Столбец: "день недели" | График t по дням недели |

---

## 5. Дополнительная аналитика (Jupyter Notebook)

После выполнения DAG и получения модели `ml_model.pkl`, необходимо провести прогнозирование продаж на основе температуры из вашего индивидуального задания.

Создайте ноутбук `прогноз_продаж.ipynb` со следующим содержимым:

### Ячейка 1: Загрузка модели
Загрузите файл `ml_model.pkl`, который был сгенерирован вашим DAG-ом (или DAG-ом `real_umbrella`).
```python
from google.colab import files
# Загрузите файл ml_model.pkl с локальной машины (куда он был сохранен Airflow)
uploaded = files.upload() 
```

### Ячейка 2: Установка зависимостей
```python
!pip install dill joblib pandas scikit-learn
```

### Ячейка 3: Прогнозирование
Используйте модель для прогноза. Вместо `15` подставьте среднюю температуру, полученную в вашем индивидуальном задании.
```python
import joblib
import pandas as pd

# Загрузка модели
# Убедитесь, что версия scikit-learn совпадает с той, на которой обучалась модель в Airflow
model = joblib.load("ml_model.pkl")

# Введите температуру из вашего варианта (например, прогноз на завтра)
my_temp = 15 

# Формирование датафрейма для предикта
input_data = pd.DataFrame({'temperature': [my_temp]})

# Предсказание объема продаж (или другой целевой метрики Umbrella)
prediction = model.predict(input_data)

print(f"При температуре {my_temp}°C прогнозируемые продажи составят: {prediction[0]:.2f}")
```

---

## 6. Отчетность и критерии оценки

**Формат сдачи:**
1.  Ссылка на Git-репозиторий с проектом (отправляется в LMS Moodle).
2.  В репозитории должны находиться:
    *   Файл отчета `ФИО.pdf` (описание шагов, скриншоты Airflow Graph View и Logs, код DAG).
    *   Файл архитектуры `archi.png`.
    *   Файл ноутбука `прогноз_продаж.ipynb` с результатами.
3.  Файл модели `ml_model.pkl`.

**Критерии оценки (Максимум 10 баллов):**

| Критерий | Баллы | Описание |
|---|---|---|
| **Развертывание среды** | 2 | Airflow запущен в Docker, DAG `real_umbrella` отработал корректно, API подключен. |
| **Архитектура** | 2 | Схема в Draw.io выполнена корректно, отражает слои Source, Storage, Business. |
| **Индивидуальное задание (ETL)** | 3 | Написан и выполнен кастомный DAG/скрипт. Данные получены, преобразованы и сохранены согласно варианту. |
| **ML Аналитика** | 2 | Выполнен перенос модели (`.pkl`) из контура ETL в контур аналитики. Ноутбук работает, прогноз получен. |
| **Оформление** | 1 | Отчет полон, репозиторий структурирован, файлы названы корректно. |
```