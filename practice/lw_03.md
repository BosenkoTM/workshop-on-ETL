# Лабораторная работа №3. Интеграция данных из разнородных источников. Проектирование архитектуры

**Цель работы.** Разработать комплексное ETL-решение для интеграции данных из локальной СУБД PostgreSQL и файловых источников (CSV/Excel) в целевое хранилище MySQL. Спроектировать верхнеуровневую архитектуру аналитического решения.

---

## 1. Техническое обеспечение и окружение

Для выполнения работы используется виртуальная машина **ETL_devops_26**.

### Необходимое ПО:
*   **ETL:** Pentaho Data Integration 9.4 (PDI).
*   **Source DBMS:** PostgreSQL (Localhost, Docker).
*   **Target DBMS:** MySQL (Remote Server).
*   **Modeling:** [Draw.io](https://app.diagrams.net/) (или десктопная версия).

### Реквизиты подключений:

**1. Источник (Source Layer): PostgreSQL**
*   **Хост:** `localhost`
*   **Порт:** `5432`
*   **База данных:** `st_200` (или иная, указанная в конфигурации Docker/pgAdmin).
*   **Логин/Пароль:** `admin`/`admin` (или см. `docker-compose.yml` на ВМ).
*   *Примечание.* Для управления используйте pgAdmin4 (`http://localhost/login/next=/`).

**2. Приемник (Storage Layer): MySQL**
*   **Хост:** `95.131.149.21`
*   **Порт:** `3306`
*   **База данных:** `mgpu_ico_etl_XX` (ваш номер группы/студента).
*   **Логин/Пароль.** Выдается преподавателем.

---

## 2. Задание Часть 1. Архитектура решения

Перед реализацией в Pentaho необходимо спроектировать схему потоков данных.

**Задание:**
Нарисуйте верхнеуровневую архитектуру вашего решения в **Draw.io**. Схема должна содержать три обязательных слоя (использовать прямоугольники-контейнеры):

1.  **Source Layer (Слой источников):**
    *   Отобразить PostgreSQL (таблицы согласно варианту).
    *   Отобразить файлы (CSV, Excel).
2.  **Storage Layer (Слой хранения):**
    *   Отобразить целевую БД MySQL.
    *   Отобразить таблицы фактов и измерений, куда будут загружены данные.
    *   Отобразить Staging Area (если используется промежуточная обработка).
3.  **Business Layer (Бизнес-слой):**
    *   Отобразить витрины данных (Views) или отчеты, которые строятся на основе данных MySQL.

*Сохраните схему как изображение (PNG/JPG) и включите в отчет.*

---

## 3. Задание Часть 2. Реализация ETL-процесса

### Шаг 1. Подготовка источника (PostgreSQL)
В pgAdmin4 подключитесь к локальному серверу PostgreSQL и создайте таблицу-источник. В качестве примера используйте скрипт `employees`, но **адаптируйте его под сущности вашего варианта** (см. раздел 4).

*Пример SQL для PostgreSQL:*
```sql
CREATE TABLE source_data (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    event_date DATE,
    amount NUMERIC(10, 2),
    category VARCHAR(50)
);
-- Вставьте тестовые данные (не менее 15 строк)
INSERT INTO source_data (name, event_date, amount, category) VALUES ...;
```

### Шаг 2. Подготовка файлов
Создайте локально файлы `.csv` и `.xlsx` с дополнительными данными согласно вашему варианту (например, справочники категорий, данные о бонусах и т.д.).

### Шаг 3. Разработка трансформации в Pentaho (Spoon)
Создайте трансформацию `.ktr`, которая выполняет:
1.  **Table Input.** Чтение данных из PostgreSQL.
2.  **CSV Input / Excel Input.** Чтение данных из файлов.
3.  **Stream Lookup / Merge Join.** Объединение данных из БД и файлов по ключевым полям.
4.  **Data Validation / Filter Rows.** Очистка данных (удаление дублей, проверка на NULL, валидация типов).
5.  **Calculator / Formula.** Расчет вычисляемых показателей (если требуется по заданию).
6.  **Table Output.** Загрузка итогового набора в вашу базу MySQL (`mgpu_ico_etl_XX`).

### Шаг 4. Создание витрины данных (MySQL View)
После загрузки данных подключитесь к MySQL (через phpMyAdmin или DBeaver) и создайте представление (`VIEW`), которое объединяет загруженные данные для бизнес-пользователя.

```sql
CREATE OR REPLACE VIEW view_analytics_report AS
SELECT 
    t1.category, 
    SUM(t1.amount) as total_sales, 
    COUNT(*) as transactions_count
FROM your_target_table t1
GROUP BY t1.category;
```

---

## 4. Варианты индивидуальных заданий

В каждом варианте необходимо интегрировать данные из трех источников: **PostgreSQL** (основные данные), **Excel** (справочники/планы), **CSV** (транзакции/логи).
*В случае недоступности специфического датасета, сгенерируйте синтетические данные.*

| № | Тема и источники | Задача (ETL + Аналитика) |
|---|---|---|
| 1 | **Продажи.** <br>PostgreSQL: Клиенты.<br>Excel: Справочник товаров.<br>CSV: Транзакции продаж. | Объединить данные, рассчитать общую сумму продаж по клиентам и категориям товаров. Выгрузить в MySQL. |
| 2 | **HR (Зарплата).** <br>PostgreSQL: Личные данные сотрудников.<br>Excel: Данные об окладах.<br>CSV: Данные о премиях. | Создать единый отчет по начислениям (оклад + премия). Рассчитать налоги. Выгрузить в MySQL таблицу выплат. |
| 3 | **Склад.** <br>PostgreSQL: Розничные магазины.<br>MySQL (исх): Основной склад.<br>Excel: Поставки. | Создать систему учета остатков. Объединить остатки склада и магазинов, выявить расхождения с поставками. |
| 4 | **Финансы.** <br>PostgreSQL: Транзакции.<br>Excel: Бюджетные лимиты.<br>CSV: Банковские выписки. | Сформировать план-фактный анализ (сравнение транзакций с бюджетом). Провести сверку движения средств. |
| 5 | **Клиентский сервис.** <br>PostgreSQL: Интернет-магазин (заказы).<br>CSV: Служба доставки (статусы).<br>Excel: Отзывы клиентов. | Создать отчет по качеству обслуживания: сопоставить время доставки с оценкой клиента в отзыве. |
| 6 | **Маркетинг.** <br>PostgreSQL: Рекламные кампании.<br>CSV: Данные из соцсетей (клики/просмотры).<br>Excel: Бюджеты на рекламу. | Проанализировать ROI (возврат инвестиций) маркетинговых каналов. Рассчитать стоимость клика. |
| 7 | **HR (Обучение).** <br>PostgreSQL: Персонал.<br>Excel: Данные об обучении/курсах.<br>CSV: Результаты аттестаций. | Создать профиль развития сотрудника. Сопоставить прохождение курсов с результатами аттестации. |
| 8 | **Производство.** <br>PostgreSQL: Производственные заказы.<br>CSV: Данные контроля качества (брак).<br>Excel: Нормативы производства. | Проанализировать эффективность производства. Выявить отклонения от нормативов и процент брака по сменам. |
| 9 | **Логистика.** <br>PostgreSQL: Перевозки (рейсы).<br>Excel: Затраты на топливо.<br>CSV: Маршрутные листы (километраж). | Оптимизировать логистические затраты. Рассчитать стоимость 1 км пробега для каждого рейса. |
| 10 | **Техподдержка.** <br>PostgreSQL: Обращения клиентов (тикетов).<br>Excel: Регламенты обслуживания (SLA).<br>CSV: Оценки качества поддержки. | Проанализировать соблюдение SLA. Выявить тикеты с нарушением сроков и низкой оценкой. |
| 11 | **Закупки.** <br>PostgreSQL: База поставщиков.<br>Excel: Ценовые предложения (прайс-листы).<br>CSV: История закупок. | Оптимизировать закупки. Сравнить цены из истории закупок с текущими прайс-листами, найти лучшего поставщика. |
| 12 | **Оборудование.** <br>PostgreSQL: Инвентаризация оборудования.<br>Excel: График обслуживания.<br>CSV: История ремонтов/поломок. | Создать систему учета ТО. Выявить оборудование с частыми поломками, нарушающее график обслуживания. |
| 13 | **Лояльность.** <br>PostgreSQL: Бонусная программа (счета).<br>CSV: История покупок.<br>Excel: Акции и спецпредложения. | Проанализировать эффективность программы лояльности. Сколько бонусов начислено/списано по акционным товарам. |
| 14 | **Образование.** <br>PostgreSQL: Студенты.<br>Excel: Оценки.<br>CSV: Посещаемость занятий. | Комплексный анализ успеваемости. Найти корреляцию между посещаемостью и средним баллом студента. |
| 15 | **Недвижимость.** <br>PostgreSQL: Объекты недвижимости.<br>Excel: Оценка состояния/ремонта.<br>CSV: История арендных платежей. | Оценить доходность объектов. Рассчитать прибыль с учетом затрат на ремонт (из оценки состояния). |
| 16 | **E-commerce.** <br>PostgreSQL: Товары (каталог).<br>CSV: Отзывы покупателей.<br>Excel: Данные о возвратах. | Анализ ассортимента. Выявить товары с высоким процентом возвратов и плохими отзывами. |
| 17 | **Страхование.** <br>PostgreSQL: Страховые полисы.<br>Excel: Страховые случаи (выплаты).<br>CSV: Оценка рисков. | Анализ портфеля. Рассчитать убыточность (отношение выплат к стоимости полиса) по категориям риска. |
| 18 | **Call-центр.** <br>PostgreSQL: Журнал звонков.<br>Excel: Оценки операторов (KPI).<br>CSV: Тематика обращений. | Оценка эффективности КЦ. Сопоставить длительность звонка с тематикой и рейтингом оператора. |
| 19 | **Управление проектами.** <br>PostgreSQL: Проекты.<br>Excel: Ресурсы и бюджеты.<br>CSV: Отчеты о выполнении (таймшиты). | Мониторинг проектов. Сравнить плановый бюджет с фактическими затратами времени сотрудников. |
| 20 | **Контроль качества.** <br>PostgreSQL: Данные контроля (тесты).<br>Excel: Нормативы ГОСТ/ISO.<br>CSV: Рекламации от клиентов. | Анализ системы качества. Сопоставить результаты внутренних тестов с реальными жалобами клиентов. |

---

## 5. Требования к отчету и критерии оценки

Отчет предоставляется в виде ссылки на репозиторий (GitHub/GitVerse). Репозиторий должен содержать:
1.  Файл **README.md** с описанием задачи, скриншотами и выводами.
2.  Файл схемы архитектуры (**Draw.io** export).
3.  Файлы трансформаций Pentaho (`.ktr`, `.kjb`).
4.  SQL-скрипты создания таблиц и View.

### Критерии оценки (Максимум 10 баллов)

| Критерий | Баллы | Описание |
|---|---|---|
| **Архитектура (Draw.io)** | 2 | Схема корректна, содержит слои Source, Storage, Business. Четко обозначены потоки данных. |
| **Подготовка среды** | 2 | В PostgreSQL созданы таблицы, данные подготовлены. В Pentaho настроены подключения к обоим СУБД. |
| **ETL реализация** | 3 | Трансформация PDI работает без ошибок. Реализованы Join разнородных источников (SQL + CSV/Excel). Применена очистка данных. |
| **Результат в MySQL** | 2 | Данные загружены в целевую таблицу. Создано SQL View (представление), решающее бизнес-задачу варианта. |
| **Отчет** | 1 | Репозиторий оформлен, присутствуют скриншоты выполнения и код. |

---

### Контрольные вопросы
1.  Какие типы Join (объединения) поддерживает PDI (Merge Join, Stream Lookup, Database Join) и в чем их отличие?
2.  Как настроить подключение JDBC к PostgreSQL в Pentaho?
3.  Для чего нужен слой Storage в архитектуре и почему нельзя строить аналитику напрямую на Source?
```