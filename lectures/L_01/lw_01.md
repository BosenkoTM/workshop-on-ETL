# Лабораторная работа №1. Установка и настройка ETL-инструмента. Создание конвейеров данных

**Цель работы.** Изучение основных принципов работы с ETL-инструментами на примере Pentaho Data Integration (PDI), настройка среды, создание конвейера обработки данных (фильтрация, очистка, замена значений) и выгрузка результатов в базу данных MySQL.

---

## 1. Техническое обеспечение и окружение

Для выполнения работы используется виртуальная машина **ETL_devops_26** (образ Ubuntu 22.04).

### Необходимое ПО:
*   **ОС:** Ubuntu 22.04 LTS
*   **ETL:** Pentaho Data Integration 9.4 (PDI)
*   **Java:** OpenJDK 11
*   **СУБД:** MySQL (удаленный сервер)

### Реквизиты подключения к СУБД (Target DB):
Для загрузки обработанных данных используйте следующие настройки в компоненте `Table Output`:

*   **Хост (Host):** `95.131.149.21`
*   **Порт (Port):** `3306`
*   **Веб-интерфейс (phpMyAdmin):** [Ссылка](http://95.131.149.21:8080/phpmyadmin/index.php?route=/sql&pos=0&db=mgpu_ico_etl_prepod&table=orders_01)
*   **База данных:** `mgpu_ico_etl_XX` (где `XX` — номер вашей базы, выданный преподавателем, например `mgpu_ico_etl_01`).
*   **Логин/Пароль:** Запросить у ведущего преподавателя.

---

## 2. Подготовка окружения (Ubuntu 22.04)

Если вы используете чистый образ или новую установку, выполните следующие шаги в терминале.

### Шаг 1. Установка Java и зависимостей
```bash
sudo apt update
sudo apt install openjdk-11-jdk -y
java -version
```

### Шаг 2. Исправление для библиотеки WebKitGTK
Так как PDI использует старую библиотеку `libwebkitgtk`, отсутствующую в репозиториях Ubuntu 22.04, необходимо добавить репозиторий `bionic`.

1.  Откройте список источников:
    ```bash
    sudo nano /etc/apt/sources.list
    ```
2.  Добавьте строку в конец файла:
    ```text
    deb http://cz.archive.ubuntu.com/ubuntu bionic main universe
    ```
    *(Или: `deb http://mirrors.kernel.org/ubuntu bionic main universe`)*
3.  Сохраните (`Ctrl+X`, `Y`, `Enter`) и выполните команды:
    ```bash
    sudo apt update
    # Добавляем ключи, если ругается на подписи (код ключа взять из ошибки терминала)
    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 
    sudo apt update
    sudo apt install libwebkitgtk-1.0-0 -y
    ```

### Шаг 3. Установка драйвера MySQL (Важно!)
Для подключения Pentaho к MySQL 8+ требуется драйвер `mysql-connector-j`.

1.  Скачайте `.deb` пакет драйвера (Platform Independent) с [официального сайта](https://dev.mysql.com/downloads/connector/j/) или используйте команду:
    ```bash
    wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-j_9.2.0-1ubuntu22.04_all.deb
    ```
2.  Установите пакет и найдите `.jar` файл:
    ```bash
    sudo dpkg -i mysql-connector-j_*.deb
    ls /usr/share/java/mysql-connector-j-*.jar
    ```
3.  Скопируйте драйвер в папку библиотек Pentaho (`data-integration/lib`):
    ```bash
    # Путь может отличаться в зависимости от того, куда вы распаковали PDI
    cp /usr/share/java/mysql-connector-j-9.2.0.jar ~/Downloads/data-integration/lib/
    ```
4.  Настройте права (критично для запуска):
    ```bash
    chmod 644 ~/Downloads/data-integration/lib/mysql-connector-j-9.2.0.jar
    # Смените владельца на вашего пользователя (например, user или dba)
    sudo chown $USER:$USER ~/Downloads/data-integration/lib/mysql-connector-j-9.2.0.jar
    ```

### Шаг 4. Запуск Pentaho Spoon
```bash
cd ~/Downloads/data-integration/
chmod +x spoon.sh
./spoon.sh
```

---

## 3. Задание на лабораторную работу

### Общая задача
1.  Выбрать вариант задания из таблицы ниже.
2.  Скачать CSV-датасет (если ссылка Kaggle недоступна — использовать VPN, найти зеркало, сгенерировать синтетические данные или использовать анонимизированные рабочие данные).
3.  Скачать шаблоны конвейеров для примера: [GitHub Repository](https://github.com/BosenkoTM/workshop-on-ETL/tree/main/lectures/L_01).
4.  Создать трансформацию (`.ktr`), реализующую:
    *   **CSV File Input.** Чтение данных.
    *   **Filter Rows / Value Mapper / String Operations.** Очистка данных, фильтрация битых записей, замена значений.
    *   **Table Output.** Загрузка очищенных данных в таблицу MySQL в базе `mgpu_ico_etl_XX`.
5.  Проверить результат SQL-запросом через phpMyAdmin.

### Варианты заданий

| № | Описание задания | Датасет (Kaggle / Источник) |
|---|---|---|
| 1 | **Анализ розничных продаж:** фильтрация транзакций, выявление аномалий. | [Retail Sales Dataset](https://www.kaggle.com/datasets/mohammadtalib786/retail-sales-dataset) |
| 2 | **E-commerce:** очистка данных о заказах, сегментация клиентов. | [E-Commerce Dataset](https://www.kaggle.com/datasets/carrie1/ecommerce-data) |
| 3 | **Финансы:** обработка котировок, расчет показателей. | [Financial Market Data](https://www.kaggle.com/datasets/finnhub/financial-market-data) |
| 4 | **HR-аналитика:** данные о сотрудниках, расчет KPI. | [Human Resources Dataset](https://www.kaggle.com/datasets/rhuebner/human-resources-data-set) |
| 5 | **Цепочки поставок:** оптимизация логистических данных. | [Supply Chain Dataset](https://www.kaggle.com/datasets/shashwatwork/dataco-smart-supply-chain-dataset) |
| 6 | **Маркетинг:** анализ эффективности рекламных кампаний. | [Marketing Campaign Dataset](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign) |
| 7 | **Клиентский сервис:** обработка обращений клиентов. | [Customer Support Dataset](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter) |
| 8 | **Веб-аналитика:** данные о посещаемости сайта. | [Web Analytics Dataset](https://www.kaggle.com/datasets/tunguz/ga-customer-revenue-prediction) |
| 9 | **Прогнозирование спроса:** история продаж. | [Demand Forecasting Dataset](https://www.kaggle.com/datasets/felixzhao/productdemandforecasting) |
| 10 | **Банковские транзакции:** выявление паттернов. | [Banking Transactions Dataset](https://www.kaggle.com/datasets/apoorvwatsky/bank-transaction-data) |
| 11 | **Производство:** операционная эффективность. | [Manufacturing Dataset](https://www.kaggle.com/datasets/inIT-OWL/production-plant-data) |
| 12 | **Недвижимость:** очистка и трансформация данных. | [Real Estate Dataset](https://www.kaggle.com/datasets/arslanali4343/real-estate-dataset) |
| 13 | **Социальные медиа:** анализ активности. | [Social Media Dataset](https://www.kaggle.com/datasets/gokulrajkmv/social-media-sentiment-analysis) |
| 14 | **Биржа:** обработка данных торгов. | [Stock Market Dataset](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs) |
| 15 | **Телеком:** анализ использования услуг и оттока. | [Telecom Dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn) |
| 16 | **Бюджетная аналитика:** финансовые показатели города. | [Budget Analytics Dataset](https://www.kaggle.com/datasets/city-of-seattle/seattle-budget-data) |
| 17 | **Программы лояльности:** обработка бонусных данных. | [Loyalty Program Dataset](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata) |
| 18 | **Цифровая реклама:** обработка данных кампаний. | [Digital Ads Dataset](https://www.kaggle.com/datasets/vidurpunj/facebook-ad-campaign) |
| 19 | **Страхование:** риски и страховые случаи. | [Insurance Risk Dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance) |
| 20 | **Инвестиции:** инвестиционные портфели. | [Investment Portfolio Dataset](https://www.kaggle.com/datasets/stefanoleone992/mutual-funds-and-etfs) |

> **Примечание.** Если датасет недоступен, допускается использование любого схожего открытого набора данных или синтетических данных.

---

## 4. Отчетность и требования к сдаче

### Формат отчета
Отчет оформляется в репозитории на **GitHub** или **GitVerse**. В системе Moodle прикрепляется **только ссылка** на репозиторий.

Структура репозитория:
1.  **Файл `README.md`** содержащий:
    *   Номер варианта и название задачи.
    *   Описание входных данных.
    *   Скриншоты созданного конвейера в Spoon (общий вид).
    *   Скриншоты настроек ключевых шагов (Input, Filter, Output).
    *   SQL-запросы, использованные для проверки загрузки данных, и скриншот результата `SELECT` из phpMyAdmin.
2.  **Файл трансформации** (`lab_01.ktr`).
3.  **Исходный файл данных** (`.csv`) или ссылка на него.

### Критерии оценки (Максимум 10 баллов)

| Критерий | Баллы | Описание |
|---|---|---|
| **Настройка среды** | 2 | Успешный запуск PDI, настройка драйвера MySQL, успешное соединение с БД (Test connection). |
| **ETL-конвейер** | 4 | Реализован полный цикл: Чтение -> Преобразование (не менее 2 шагов) -> Запись. |
| **Результат в БД** | 2 | Таблица в MySQL создана автоматически или вручную, данные загружены корректно (кодировка, типы данных). |
| **Отчет** | 2 | Репозиторий оформлен аккуратно, присутствуют все необходимые скриншоты и файлы, SQL-проверка. |

---

### Подсказки по SQL
Для проверки количества загруженных строк:
```sql
SELECT COUNT(*) FROM ваша_таблица;
```
Для просмотра первых 10 записей:
```sql
SELECT * FROM ваша_таблица LIMIT 10;
```
